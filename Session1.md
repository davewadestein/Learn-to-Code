# Summary of Session 1: Jargon!
 * __bit__ = "binary digit" = a 0 or 1
   * the smallest unit of data that a computer can process/store
 * __byte__ = 8 bits = 1 character (on a US keyboard)
   * megabyte (MB) = 1 Million bytes (technically it's 2^20 = 1,048,576 bytes
   * gigabyte (GB) = 1 Billion bytes (2^30 = 1,073,741,824 bytes)
   * terabyte (TB) = 1 Trillion bytes (2^40 = 1,099,511,627,776 bytes)
 * __CPU__ = Central Processing Unit = "brain" of a computer
   * it's a _super_ fast, _super_ dumb brain
   * it can do billions of operations per second
   * ...but each operation is pretty simple
     * moving data from memory into the CPU
     * storing data in the CPU into memory
     * integer arithmetic (e.g., 2 + 3)
     * floating point arithmetic (e.g., 2.3 * -5.1)
     * branching ("jump" to a different place in the code)
  * __RAM__ = Random Access Memory (or just "memory")
    * space the computer can work with while it's turned on (e.g., applications live in RAM when they are running)
    * 16 GB RAM = 16 Billion bytes or characters worth of "work space"
  
 * to be continued
